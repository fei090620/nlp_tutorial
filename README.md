# nlp_tutorial
## 前言
一直以来对于NLP都处于一个比较迷糊的状态，自然语言处理，到底是怎么处理的，有哪些常用的方法和思路，恰逢项目需要，借由本篇教程梳理NLP解决实际问题的一般思路。

这篇教程期待你本身对于NLP的知识有一定的了解，知晓NLP包括哪些部分（词，句，段，篇章等），知晓NLP有哪些应用（相似度，分类，生成等），反正，你从这个项目里是看不到这些基础知识的。

相反，这里会给你描述一般情况下，项目中NLP需要面对的业务场景和需要解决的问题，以及可以采取的思路和方案，当然，还有亲自实践，有code。



## NLP面临的常见业务场景

巨量的文本信息，可能是短文本，几句话；也可能是长文本，文章或新闻报道。而且通常样本没有标注，不读其文，难知其义，更难知其类别。而真实的需求，往往和业务紧密结合，需要从这些文本信息中获取某些有价值的业务数据，获取某种隐藏的规律等等。



## 具体场景中NLP的通用思路

没有标注的巨量文本数据，首先需要解决的就是文本的结构化。长文本由段落组成，段落其实相当于短文本，段落或者短文本由句子组成，句子则有词组成，词是NLP中的最小处理单元，所以NLP的第一步，往往是分词。

### 分词

分词的方法有多种，有基于词典的，有基于统计概率的，理论算法的描述网上不胜枚举，也可以看看吴军先生的《数学之美》。这里只告诉你，项目中应该怎么分词，有哪些开源项目可以帮助你。

实际项目中分词的过程可能是最最重要的，虽然，分词的算法不算很难。分词，其实在为后续的处理准备最初级的原材料，这一步如果贴近业务，准确度高，就决定了后续的每一步都有一个可靠的基础，相反NLP的每一步都或多或少带有误差累积的顽疾，如果分词的准确度不高，后续的NLP处理也极难有较好的结果。

原始分词，工具可以使用[jieba分词](https://github.com/fxsjy/jieba)，当然，也可以使用[HanLP](https://github.com/hankcs/HanLP)，其中jieba是python实现的，HanLP是Java实现的，本身没有绝对的鸿沟，使用那个，随意就好。

关键是，分词工具一定要支持自定义词库！



###未完待续

